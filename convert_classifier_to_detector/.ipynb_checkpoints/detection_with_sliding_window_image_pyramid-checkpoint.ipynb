{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce95e24",
   "metadata": {},
   "source": [
    "## Converting classifier to detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c28c4c",
   "metadata": {},
   "source": [
    "### Learning and code taken from Adrian's Tutorial on Object detcetion :\n",
    "https://pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de2a0f",
   "metadata": {},
   "source": [
    "### 1. Using sliding window \n",
    "####  a.Use sliding window to get the ROI (location) and feed that into your CNN classifier\n",
    "####  b. No change ine the deep neural network\n",
    "####   c. Need to implement a preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de12f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a19fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b80a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step : stride for sliding the window across image (recommendation is 4 to 8 pixels)\n",
    "# ws: The ROI or window size for cropping out the image and feeding to the CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79f1065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 479, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path=r\"D:\\LearningObjectDetection\\images\"\n",
    "image_path = os.path.join(base_path,\"dog1.jpg\")\n",
    "img = cv2.imread(image_path)\n",
    "print(img.shape)\n",
    "cv2.imshow(\"img\", img)\n",
    "cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a923e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step=16\n",
    "window_size=(224,224)\n",
    "img_resize_width=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa22a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables used for the object detection procedure\n",
    "WIDTH = 600\n",
    "PYR_SCALE = 1.5\n",
    "WIN_STEP = 16\n",
    "#ROI_SIZE = eval(args[\"size\"])\n",
    "INPUT_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56aa51e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "16 0\n",
      "32 0\n",
      "48 0\n",
      "64 0\n",
      "80 0\n",
      "96 0\n",
      "112 0\n",
      "128 0\n",
      "144 0\n",
      "160 0\n",
      "176 0\n",
      "192 0\n",
      "208 0\n",
      "224 0\n",
      "240 0\n",
      "0 16\n",
      "16 16\n",
      "32 16\n",
      "48 16\n",
      "64 16\n",
      "80 16\n",
      "96 16\n",
      "112 16\n",
      "128 16\n",
      "144 16\n",
      "160 16\n",
      "176 16\n",
      "192 16\n",
      "208 16\n",
      "224 16\n",
      "240 16\n",
      "0 32\n",
      "16 32\n",
      "32 32\n",
      "48 32\n",
      "64 32\n",
      "80 32\n",
      "96 32\n",
      "112 32\n",
      "128 32\n",
      "144 32\n",
      "160 32\n",
      "176 32\n",
      "192 32\n",
      "208 32\n",
      "224 32\n",
      "240 32\n",
      "0 48\n",
      "16 48\n",
      "32 48\n",
      "48 48\n",
      "64 48\n",
      "80 48\n",
      "96 48\n",
      "112 48\n",
      "128 48\n",
      "144 48\n",
      "160 48\n",
      "176 48\n",
      "192 48\n",
      "208 48\n",
      "224 48\n",
      "240 48\n",
      "0 64\n",
      "16 64\n",
      "32 64\n",
      "48 64\n",
      "64 64\n",
      "80 64\n",
      "96 64\n",
      "112 64\n",
      "128 64\n",
      "144 64\n",
      "160 64\n",
      "176 64\n",
      "192 64\n",
      "208 64\n",
      "224 64\n",
      "240 64\n",
      "0 80\n",
      "16 80\n",
      "32 80\n",
      "48 80\n",
      "64 80\n",
      "80 80\n",
      "96 80\n",
      "112 80\n",
      "128 80\n",
      "144 80\n",
      "160 80\n",
      "176 80\n",
      "192 80\n",
      "208 80\n",
      "224 80\n",
      "240 80\n",
      "0 96\n",
      "16 96\n",
      "32 96\n",
      "48 96\n",
      "64 96\n",
      "80 96\n",
      "96 96\n",
      "112 96\n",
      "128 96\n",
      "144 96\n",
      "160 96\n",
      "176 96\n",
      "192 96\n",
      "208 96\n",
      "224 96\n",
      "240 96\n",
      "0 112\n",
      "16 112\n",
      "32 112\n",
      "48 112\n",
      "64 112\n",
      "80 112\n",
      "96 112\n",
      "112 112\n",
      "128 112\n",
      "144 112\n",
      "160 112\n",
      "176 112\n",
      "192 112\n",
      "208 112\n",
      "224 112\n",
      "240 112\n",
      "0 128\n",
      "16 128\n",
      "32 128\n",
      "48 128\n",
      "64 128\n",
      "80 128\n",
      "96 128\n",
      "112 128\n",
      "128 128\n",
      "144 128\n",
      "160 128\n",
      "176 128\n",
      "192 128\n",
      "208 128\n",
      "224 128\n",
      "240 128\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'destroAllWindows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27456/3262392519.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"img\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_vis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'destroAllWindows'"
     ]
    }
   ],
   "source": [
    "for x,y,img_crop in sliding_window(img, step, window_size):\n",
    "    print(x,y)\n",
    "    img_vis=img.copy()\n",
    "    cv2.rectangle(img_vis, (x, y), (x+window_size[0], y+window_size[0]), (255,0,0), 2)\n",
    "    cv2.imshow(\"img\", img_vis)\n",
    "    cv2.waitKey(30)\n",
    "cv2.destroAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc2b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing the resnet model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c64249",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_size=(224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ffd1cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n",
      "[INFO] image shape:  (360, 479, 3)\n",
      "[INFO] image shape after resize:  (224, 224, 3)\n",
      "[INFO] image shape after preprocessing:  (1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading network...\")\n",
    "model = ResNet50(weights=\"imagenet\", include_top=True)\n",
    "# load the input image from disk, resize it such that it has the\n",
    "# has the supplied width, and then grab its dimensions\n",
    "img = cv2.imread(image_path)\n",
    "print(\"[INFO] image shape: \",img.shape)\n",
    "img = cv2.resize(img, model_input_size)\n",
    "#img = imutils.resize(img, width=img_resize_width)\n",
    "print(\"[INFO] image shape after resize: \",img.shape)\n",
    "img = img_to_array(img)\n",
    "img = preprocess_input(img)\n",
    "img = np.expand_dims(img,axis=0)\n",
    "print(\"[INFO] image shape after preprocessing: \",img.shape)\n",
    "\n",
    "\n",
    "#(H, W) = img.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de487b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying the image...\n",
      "1/1 [==============================] - 1s 721ms/step\n",
      "[INFO] classifying image took 0.76872 seconds\n",
      "[INFO] Prediction values: \n",
      "num prediction:  1000\n",
      "max value and class number:  0.3041564 and 194\n",
      "prediction decoded: Imagenet ID, Class Name, Class Prob\n",
      "[[('n02096437', 'Dandie_Dinmont', 0.3041564)]]\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] classifying the image...\")\n",
    "start = time.time()\n",
    "preds = model.predict(img)\n",
    "end = time.time()\n",
    "print(\"[INFO] classifying image took {:.5f} seconds\".format(end - start))\n",
    "print(\"[INFO] Prediction values: \")\n",
    "print(\"num prediction: \",len(preds[0]))\n",
    "#print(preds)\n",
    "print(\"max value and class number: \",np.max(preds[0]),\"and\",np.argmax(preds[0]))\n",
    "# decode the predictions and initialize a dictionary which maps class\n",
    "# labels (keys) to any ROIs associated with that label (values)\n",
    "preds = imagenet_utils.decode_predictions(preds, top=1)\n",
    "print (\"prediction decoded: Imagenet ID, Class Name, Class Prob\")\n",
    "print(preds)\n",
    "labels = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea37a46",
   "metadata": {},
   "source": [
    "### Using Sliding window and ResNet50 for object detection\n",
    "### Many of the ROIs will be simply background so wee need to filter out those based on threshold probbility score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee465880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_conf=0.6\n",
    "min_conf=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85fb6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the sliding window locations and store each region and its location\n",
    "rois=[]\n",
    "locs=[]\n",
    "img = cv2.imread(image_path)\n",
    "for (x, y, roi_cropped) in sliding_window(img,  step, window_size):\n",
    "\n",
    "    w=window_size[0]\n",
    "    h=window_size[1]\n",
    "    # take the ROI and preprocess it so we can later classify\n",
    "    # the region using Keras/TensorFlow\n",
    "    roi = cv2.resize(roi_cropped, model_input_size)\n",
    "    roi = img_to_array(roi)\n",
    "    roi = preprocess_input(roi)\n",
    "    # update our list of ROIs and associated coordinates\n",
    "    rois.append(roi)\n",
    "    locs.append((x, y, x+w, y+h))\n",
    "    \n",
    "    img_vis=img.copy()\n",
    "    img_roi=roi_cropped.copy()\n",
    "    cv2.rectangle(img_vis, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "    cv2.imshow(\"img\", img_vis)\n",
    "    cv2.imshow(\"cropped_roi\", img_roi)\n",
    "    cv2.waitKey(30)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583817a6",
   "metadata": {},
   "source": [
    "#### Do the prediction on all rois as a batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de376ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying ROIs...\n",
      "5/5 [==============================] - 6s 1s/step\n",
      "[INFO] classifying 144 ROIs took 5.62277 seconds\n"
     ]
    }
   ],
   "source": [
    "# convert the ROIs to a NumPy array\n",
    "rois = np.array(rois, dtype=\"float32\")\n",
    "# classify each of the proposal ROIs using ResNet and then show how\n",
    "# long the classifications took\n",
    "print(\"[INFO] classifying ROIs...\")\n",
    "start = time.time()\n",
    "preds = model.predict(rois)\n",
    "end = time.time()\n",
    "print(\"[INFO] classifying {} ROIs took {:.5f} seconds\".format(rois.shape[0],end - start))\n",
    "# decode the predictions and initialize a dictionary which maps class\n",
    "# labels (keys) to any ROIs associated with that label (values)\n",
    "preds = imagenet_utils.decode_predictions(preds, top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e853f0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('n02096437', 'Dandie_Dinmont', 0.7581882)], [('n02096437', 'Dandie_Dinmont', 0.7054756)], [('n02096437', 'Dandie_Dinmont', 0.81746554)], [('n02096437', 'Dandie_Dinmont', 0.6715009)], [('n02096437', 'Dandie_Dinmont', 0.86994225)], [('n02096437', 'Dandie_Dinmont', 0.703116)], [('n02096437', 'Dandie_Dinmont', 0.8541621)], [('n02096437', 'Dandie_Dinmont', 0.65726435)], [('n02096437', 'Dandie_Dinmont', 0.80990857)], [('n02096437', 'Dandie_Dinmont', 0.6726562)], [('n02096437', 'Dandie_Dinmont', 0.4215826)], [('n02113624', 'toy_poodle', 0.504241)], [('n02113624', 'toy_poodle', 0.43144512)], [('n02113712', 'miniature_poodle', 0.47269338)], [('n02113712', 'miniature_poodle', 0.6567762)], [('n02113712', 'miniature_poodle', 0.35528263)], [('n02113624', 'toy_poodle', 0.4177775)], [('n02113624', 'toy_poodle', 0.39775142)], [('n02113624', 'toy_poodle', 0.2925979)], [('n02113624', 'toy_poodle', 0.3537338)], [('n02096437', 'Dandie_Dinmont', 0.3451155)], [('n02113624', 'toy_poodle', 0.49088678)], [('n02113624', 'toy_poodle', 0.38234988)], [('n02113624', 'toy_poodle', 0.50027317)], [('n02113624', 'toy_poodle', 0.3988207)], [('n02113624', 'toy_poodle', 0.53995216)], [('n02113624', 'toy_poodle', 0.5930805)], [('n02113624', 'toy_poodle', 0.5888856)], [('n02085936', 'Maltese_dog', 0.417127)], [('n02113624', 'toy_poodle', 0.5134832)], [('n02113712', 'miniature_poodle', 0.6467027)], [('n02113624', 'toy_poodle', 0.3907969)], [('n02096437', 'Dandie_Dinmont', 0.6465704)], [('n02096437', 'Dandie_Dinmont', 0.68388885)], [('n02096437', 'Dandie_Dinmont', 0.6512939)], [('n02096437', 'Dandie_Dinmont', 0.45129362)], [('n02096437', 'Dandie_Dinmont', 0.49719062)], [('n02113624', 'toy_poodle', 0.46512544)], [('n02096437', 'Dandie_Dinmont', 0.38214234)], [('n02113624', 'toy_poodle', 0.49957573)], [('n02113624', 'toy_poodle', 0.40601414)], [('n02113624', 'toy_poodle', 0.51640755)], [('n02113624', 'toy_poodle', 0.5919845)], [('n02113624', 'toy_poodle', 0.67065996)], [('n02113624', 'toy_poodle', 0.5535177)], [('n02113624', 'toy_poodle', 0.55372775)], [('n02113712', 'miniature_poodle', 0.7332178)], [('n02113712', 'miniature_poodle', 0.53675884)], [('n02113624', 'toy_poodle', 0.62862545)], [('n02113624', 'toy_poodle', 0.56814563)], [('n02113624', 'toy_poodle', 0.6224773)], [('n02113624', 'toy_poodle', 0.53868026)], [('n02113624', 'toy_poodle', 0.5097343)], [('n02113624', 'toy_poodle', 0.6683977)], [('n02113624', 'toy_poodle', 0.5891252)], [('n02113624', 'toy_poodle', 0.67270964)], [('n02113624', 'toy_poodle', 0.59478897)], [('n02113624', 'toy_poodle', 0.697892)], [('n02113624', 'toy_poodle', 0.700827)], [('n02113624', 'toy_poodle', 0.70452255)], [('n02113624', 'toy_poodle', 0.56419736)], [('n02113624', 'toy_poodle', 0.5400762)], [('n02113712', 'miniature_poodle', 0.55080146)], [('n02113712', 'miniature_poodle', 0.46240038)], [('n02113624', 'toy_poodle', 0.5525167)], [('n02113624', 'toy_poodle', 0.4080764)], [('n02113624', 'toy_poodle', 0.49575728)], [('n02113624', 'toy_poodle', 0.40694255)], [('n02113624', 'toy_poodle', 0.6219096)], [('n02113624', 'toy_poodle', 0.6585403)], [('n02113624', 'toy_poodle', 0.6638394)], [('n02113624', 'toy_poodle', 0.67099684)], [('n02113624', 'toy_poodle', 0.6739839)], [('n02113624', 'toy_poodle', 0.68712455)], [('n02113624', 'toy_poodle', 0.65832865)], [('n02113624', 'toy_poodle', 0.7307197)], [('n02113624', 'toy_poodle', 0.674569)], [('n02113624', 'toy_poodle', 0.56954026)], [('n02113712', 'miniature_poodle', 0.6773389)], [('n02113712', 'miniature_poodle', 0.4233778)], [('n02113624', 'toy_poodle', 0.71469706)], [('n02113624', 'toy_poodle', 0.6691529)], [('n02113624', 'toy_poodle', 0.70059955)], [('n02113624', 'toy_poodle', 0.6783469)], [('n02113624', 'toy_poodle', 0.67242455)], [('n02113624', 'toy_poodle', 0.7538923)], [('n02113624', 'toy_poodle', 0.7048633)], [('n02113624', 'toy_poodle', 0.73302555)], [('n02113624', 'toy_poodle', 0.7033441)], [('n02113624', 'toy_poodle', 0.73722273)], [('n02113624', 'toy_poodle', 0.7098793)], [('n02113624', 'toy_poodle', 0.77493685)], [('n02113624', 'toy_poodle', 0.66879386)], [('n02113624', 'toy_poodle', 0.63855696)], [('n02113712', 'miniature_poodle', 0.56697875)], [('n02113624', 'toy_poodle', 0.49220642)], [('n02113624', 'toy_poodle', 0.7202527)], [('n02113624', 'toy_poodle', 0.52208877)], [('n02113624', 'toy_poodle', 0.7342409)], [('n02113624', 'toy_poodle', 0.6048326)], [('n02113624', 'toy_poodle', 0.72087985)], [('n02113624', 'toy_poodle', 0.7343895)], [('n02113624', 'toy_poodle', 0.7062558)], [('n02113624', 'toy_poodle', 0.71166307)], [('n02113624', 'toy_poodle', 0.6791705)], [('n02113624', 'toy_poodle', 0.702557)], [('n02113624', 'toy_poodle', 0.66341907)], [('n02113624', 'toy_poodle', 0.7614663)], [('n02113624', 'toy_poodle', 0.65908504)], [('n02113624', 'toy_poodle', 0.67100614)], [('n02113712', 'miniature_poodle', 0.6168241)], [('n02113624', 'toy_poodle', 0.43252778)], [('n02113624', 'toy_poodle', 0.8259578)], [('n02113624', 'toy_poodle', 0.8146999)], [('n02113624', 'toy_poodle', 0.8864109)], [('n02113624', 'toy_poodle', 0.8081055)], [('n02113624', 'toy_poodle', 0.8597022)], [('n02113624', 'toy_poodle', 0.8393654)], [('n02113624', 'toy_poodle', 0.8550348)], [('n02113624', 'toy_poodle', 0.8150234)], [('n02113624', 'toy_poodle', 0.8526734)], [('n02113624', 'toy_poodle', 0.772449)], [('n02113624', 'toy_poodle', 0.82090956)], [('n02113624', 'toy_poodle', 0.8422101)], [('n02113624', 'toy_poodle', 0.75495833)], [('n02113624', 'toy_poodle', 0.687938)], [('n02113712', 'miniature_poodle', 0.60286546)], [('n02113624', 'toy_poodle', 0.42984328)], [('n02113624', 'toy_poodle', 0.5851299)], [('n02113624', 'toy_poodle', 0.6623529)], [('n02113624', 'toy_poodle', 0.7739077)], [('n02113624', 'toy_poodle', 0.66503507)], [('n02113624', 'toy_poodle', 0.8402653)], [('n02113624', 'toy_poodle', 0.8345108)], [('n02113624', 'toy_poodle', 0.84165746)], [('n02113624', 'toy_poodle', 0.8135096)], [('n02113624', 'toy_poodle', 0.84049195)], [('n02113624', 'toy_poodle', 0.79017216)], [('n02113624', 'toy_poodle', 0.78309107)], [('n02113624', 'toy_poodle', 0.8285572)], [('n02113624', 'toy_poodle', 0.7332443)], [('n02113624', 'toy_poodle', 0.68736684)], [('n02113712', 'miniature_poodle', 0.59224886)], [('n02113712', 'miniature_poodle', 0.34375837)]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67c5de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "# loop over the predictions\n",
    "for (i, p) in enumerate(preds):\n",
    "\t# grab the prediction information for the current ROI\n",
    "\t(imagenetID, label, prob) = p[0]\n",
    "\t# filter out weak detections by ensuring the predicted probability\n",
    "\t# is greater than the minimum probability\n",
    "\tif prob >= min_conf:\n",
    "\t\t# grab the bounding box associated with the prediction and\n",
    "\t\t# convert the coordinates\n",
    "\t\tbox = locs[i]\n",
    "\t\t# grab the list of predictions for the label and add the\n",
    "\t\t# bounding box and probability to the list\n",
    "\t\tL = labels.get(label, [])\n",
    "\t\tL.append((box, prob))\n",
    "\t\tlabels[label] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f1a3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miniature_poodle\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(label)\n",
    "L = labels.get(label, [])\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e7737f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dandie_Dinmont': [((32, 0, 256, 224), 0.81746554),\n",
       "  ((64, 0, 288, 224), 0.86994225),\n",
       "  ((96, 0, 320, 224), 0.8541621),\n",
       "  ((128, 0, 352, 224), 0.80990857)],\n",
       " 'toy_poodle': [((0, 112, 224, 336), 0.8259578),\n",
       "  ((16, 112, 240, 336), 0.8146999),\n",
       "  ((32, 112, 256, 336), 0.8864109),\n",
       "  ((48, 112, 272, 336), 0.8081055),\n",
       "  ((64, 112, 288, 336), 0.8597022),\n",
       "  ((80, 112, 304, 336), 0.8393654),\n",
       "  ((96, 112, 320, 336), 0.8550348),\n",
       "  ((112, 112, 336, 336), 0.8150234),\n",
       "  ((128, 112, 352, 336), 0.8526734),\n",
       "  ((160, 112, 384, 336), 0.82090956),\n",
       "  ((176, 112, 400, 336), 0.8422101),\n",
       "  ((64, 128, 288, 352), 0.8402653),\n",
       "  ((80, 128, 304, 352), 0.8345108),\n",
       "  ((96, 128, 320, 352), 0.84165746),\n",
       "  ((112, 128, 336, 352), 0.8135096),\n",
       "  ((128, 128, 352, 352), 0.84049195),\n",
       "  ((176, 128, 400, 352), 0.8285572)]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12dd1ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] showing results for 'Dandie_Dinmont'\n",
      "[INFO] showing results for 'toy_poodle'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over the labels for each of detected objects in the image\n",
    "img_all_preds = img.copy()\n",
    "for label in labels.keys():\n",
    "    # clone the original image so that we can draw on it\n",
    "    print(\"[INFO] showing results for '{}'\".format(label))\n",
    "    clone = img.copy()\n",
    "    # loop over all bounding boxes for the current label\n",
    "    for (box, prob) in labels[label]:\n",
    "        # draw the bounding box on the image\n",
    "        (startX, startY, endX, endY) = box\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "        cv2.rectangle(img_all_preds, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "    cv2.imshow(\"{}\".format(label), clone)\n",
    "    #cv2.waitKey(0)\n",
    "    #clone = orig.copy()\n",
    "cv2.imshow(\"Before\", img_all_preds)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3051e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying NMS to get rid of boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3984d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] showing results for 'Dandie_Dinmont'\n",
      "[INFO] showing results for 'toy_poodle'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over the labels for each of detected objects in the image\n",
    "img_all_preds_bf=img.copy()\n",
    "img_all_preds_af=img.copy()\n",
    "\n",
    "for label in labels.keys():\n",
    "    # clone the original image so that we can draw on it\n",
    "    print(\"[INFO] showing results for '{}'\".format(label))\n",
    "    clone = img.copy()\n",
    "    # loop over all bounding boxes for the current label\n",
    "    for (box, prob) in labels[label]:\n",
    "        # draw the bounding box on the image\n",
    "        (startX, startY, endX, endY) = box\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "        cv2.rectangle(img_all_preds_bf, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "    cv2.imshow(\"{} Before\".format(label), clone)\n",
    "    \n",
    "    # extract the bounding boxes and associated prediction robabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba,overlapThresh=0.2)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    clone = img.copy()\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        cv2.putText(clone, str(endX-startx), (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.rectangle(img_all_preds_af, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "    # show the output after apply non-maxima suppression\n",
    "    cv2.imshow(\"{} After\".format(label), clone)\n",
    "\n",
    "    #cv2.waitKey(0)\n",
    "    #clone = orig.copy()\n",
    "cv2.imshow(\"Before NMS\", img_all_preds_bf)\n",
    "cv2.imshow(\"After NMS\", img_all_preds_af)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72efb90",
   "metadata": {},
   "source": [
    "#### we see false positives here whihc we can reduce by increasing the min confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06ab7a",
   "metadata": {},
   "source": [
    "## Image Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e294bb6",
   "metadata": {},
   "source": [
    "#### Utilizing an image pyramid allows us to find objects in images at different scales (sizes) of an image. And when combined with a sliding window we can find objects in images in various locations.\n",
    "#### At the bottom of the pyramid, we have the original image at its original size (in terms of width and height). And at each subsequent layer, the image is resized (subsampled) and optionally smoothed (usually via Gaussian blurring). The image is progressively subsampled until some stopping criterion is met, which is normally when a minimum size has been reached and no further subsampling needs to take place.\n",
    "#### REf: https://pyimagesearch.com/2015/03/16/image-pyramids-with-python-and-opencv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688c4a9",
   "metadata": {},
   "source": [
    "#### Combined with image pyramids, sliding windows allow us to localize objects at different locations and multiple scales of the input image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca24e5e",
   "metadata": {},
   "source": [
    "### We need to define two parameters\n",
    "### scale: by what scale images are subsample\n",
    "### minimum size of the image for stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caa2c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we are going to run teh sliding window for each scale of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "906e85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "\t# yield the original image\n",
    "\tyield image\n",
    "\t# keep looping over the image pyramid\n",
    "\twhile True:\n",
    "\t\t# compute the dimensions of the next image in the pyramid\n",
    "\t\tw = int(image.shape[1] / scale)\n",
    "\t\timage = imutils.resize(image, width=w)\n",
    "\t\t# if the resized image does not meet the supplied minimum\n",
    "\t\t# size, then stop constructing the pyramid\n",
    "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "\t\t\tbreak\n",
    "\t\t# yield the next image in the pyramid\n",
    "\t\tyield image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01ab9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables used for the object detection procedure\n",
    "base_img_width = 600\n",
    "pyr_scale = 1.5\n",
    "min_size=(224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dba5fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] image shape:  (360, 479, 3)\n",
      "[INFO] image shape after resize:  (450, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(image_path)\n",
    "print(\"[INFO] image shape: \",img.shape)\n",
    "img = imutils.resize(img, width=img_resize_width)\n",
    "img_h,img_w=img.shape[:2]\n",
    "print(\"[INFO] image shape after resize: \",img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa959f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 600, 3)\n",
      "(300, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "for img_pr in image_pyramid(img,scale=1.5,minSize=min_size):\n",
    "    print(img_pr.shape)\n",
    "    cv2.imshow(\"img_pr\",img_pr)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171942ed",
   "metadata": {},
   "source": [
    "### Combine image pyramid with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8827c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_size=(224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bf7cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org:  0 0 224 224\n",
      "scaled:  0 0 224 224\n",
      "org:  16 0 224 224\n",
      "scaled:  16 0 224 224\n",
      "org:  32 0 224 224\n",
      "scaled:  32 0 224 224\n",
      "org:  48 0 224 224\n",
      "scaled:  48 0 224 224\n",
      "org:  64 0 224 224\n",
      "scaled:  64 0 224 224\n",
      "org:  80 0 224 224\n",
      "scaled:  80 0 224 224\n",
      "org:  96 0 224 224\n",
      "scaled:  96 0 224 224\n",
      "org:  112 0 224 224\n",
      "scaled:  112 0 224 224\n",
      "org:  128 0 224 224\n",
      "scaled:  128 0 224 224\n",
      "org:  144 0 224 224\n",
      "scaled:  144 0 224 224\n",
      "org:  160 0 224 224\n",
      "scaled:  160 0 224 224\n",
      "org:  176 0 224 224\n",
      "scaled:  176 0 224 224\n",
      "org:  192 0 224 224\n",
      "scaled:  192 0 224 224\n",
      "org:  208 0 224 224\n",
      "scaled:  208 0 224 224\n",
      "org:  224 0 224 224\n",
      "scaled:  224 0 224 224\n",
      "org:  240 0 224 224\n",
      "scaled:  240 0 224 224\n",
      "org:  256 0 224 224\n",
      "scaled:  256 0 224 224\n",
      "org:  272 0 224 224\n",
      "scaled:  272 0 224 224\n",
      "org:  288 0 224 224\n",
      "scaled:  288 0 224 224\n",
      "org:  304 0 224 224\n",
      "scaled:  304 0 224 224\n",
      "org:  320 0 224 224\n",
      "scaled:  320 0 224 224\n",
      "org:  336 0 224 224\n",
      "scaled:  336 0 224 224\n",
      "org:  352 0 224 224\n",
      "scaled:  352 0 224 224\n",
      "org:  368 0 224 224\n",
      "scaled:  368 0 224 224\n",
      "org:  0 16 224 224\n",
      "scaled:  0 16 224 224\n",
      "org:  16 16 224 224\n",
      "scaled:  16 16 224 224\n",
      "org:  32 16 224 224\n",
      "scaled:  32 16 224 224\n",
      "org:  48 16 224 224\n",
      "scaled:  48 16 224 224\n",
      "org:  64 16 224 224\n",
      "scaled:  64 16 224 224\n",
      "org:  80 16 224 224\n",
      "scaled:  80 16 224 224\n",
      "org:  96 16 224 224\n",
      "scaled:  96 16 224 224\n",
      "org:  112 16 224 224\n",
      "scaled:  112 16 224 224\n",
      "org:  128 16 224 224\n",
      "scaled:  128 16 224 224\n",
      "org:  144 16 224 224\n",
      "scaled:  144 16 224 224\n",
      "org:  160 16 224 224\n",
      "scaled:  160 16 224 224\n",
      "org:  176 16 224 224\n",
      "scaled:  176 16 224 224\n",
      "org:  192 16 224 224\n",
      "scaled:  192 16 224 224\n",
      "org:  208 16 224 224\n",
      "scaled:  208 16 224 224\n",
      "org:  224 16 224 224\n",
      "scaled:  224 16 224 224\n",
      "org:  240 16 224 224\n",
      "scaled:  240 16 224 224\n",
      "org:  256 16 224 224\n",
      "scaled:  256 16 224 224\n",
      "org:  272 16 224 224\n",
      "scaled:  272 16 224 224\n",
      "org:  288 16 224 224\n",
      "scaled:  288 16 224 224\n",
      "org:  304 16 224 224\n",
      "scaled:  304 16 224 224\n",
      "org:  320 16 224 224\n",
      "scaled:  320 16 224 224\n",
      "org:  336 16 224 224\n",
      "scaled:  336 16 224 224\n",
      "org:  352 16 224 224\n",
      "scaled:  352 16 224 224\n",
      "org:  368 16 224 224\n",
      "scaled:  368 16 224 224\n",
      "org:  0 32 224 224\n",
      "scaled:  0 32 224 224\n",
      "org:  16 32 224 224\n",
      "scaled:  16 32 224 224\n",
      "org:  32 32 224 224\n",
      "scaled:  32 32 224 224\n",
      "org:  48 32 224 224\n",
      "scaled:  48 32 224 224\n",
      "org:  64 32 224 224\n",
      "scaled:  64 32 224 224\n",
      "org:  80 32 224 224\n",
      "scaled:  80 32 224 224\n",
      "org:  96 32 224 224\n",
      "scaled:  96 32 224 224\n",
      "org:  112 32 224 224\n",
      "scaled:  112 32 224 224\n",
      "org:  128 32 224 224\n",
      "scaled:  128 32 224 224\n",
      "org:  144 32 224 224\n",
      "scaled:  144 32 224 224\n",
      "org:  160 32 224 224\n",
      "scaled:  160 32 224 224\n",
      "org:  176 32 224 224\n",
      "scaled:  176 32 224 224\n",
      "org:  192 32 224 224\n",
      "scaled:  192 32 224 224\n",
      "org:  208 32 224 224\n",
      "scaled:  208 32 224 224\n",
      "org:  224 32 224 224\n",
      "scaled:  224 32 224 224\n",
      "org:  240 32 224 224\n",
      "scaled:  240 32 224 224\n",
      "org:  256 32 224 224\n",
      "scaled:  256 32 224 224\n",
      "org:  272 32 224 224\n",
      "scaled:  272 32 224 224\n",
      "org:  288 32 224 224\n",
      "scaled:  288 32 224 224\n",
      "org:  304 32 224 224\n",
      "scaled:  304 32 224 224\n",
      "org:  320 32 224 224\n",
      "scaled:  320 32 224 224\n",
      "org:  336 32 224 224\n",
      "scaled:  336 32 224 224\n",
      "org:  352 32 224 224\n",
      "scaled:  352 32 224 224\n",
      "org:  368 32 224 224\n",
      "scaled:  368 32 224 224\n",
      "org:  0 48 224 224\n",
      "scaled:  0 48 224 224\n",
      "org:  16 48 224 224\n",
      "scaled:  16 48 224 224\n",
      "org:  32 48 224 224\n",
      "scaled:  32 48 224 224\n",
      "org:  48 48 224 224\n",
      "scaled:  48 48 224 224\n",
      "org:  64 48 224 224\n",
      "scaled:  64 48 224 224\n",
      "org:  80 48 224 224\n",
      "scaled:  80 48 224 224\n",
      "org:  96 48 224 224\n",
      "scaled:  96 48 224 224\n",
      "org:  112 48 224 224\n",
      "scaled:  112 48 224 224\n",
      "org:  128 48 224 224\n",
      "scaled:  128 48 224 224\n",
      "org:  144 48 224 224\n",
      "scaled:  144 48 224 224\n",
      "org:  160 48 224 224\n",
      "scaled:  160 48 224 224\n",
      "org:  176 48 224 224\n",
      "scaled:  176 48 224 224\n",
      "org:  192 48 224 224\n",
      "scaled:  192 48 224 224\n",
      "org:  208 48 224 224\n",
      "scaled:  208 48 224 224\n",
      "org:  224 48 224 224\n",
      "scaled:  224 48 224 224\n",
      "org:  240 48 224 224\n",
      "scaled:  240 48 224 224\n",
      "org:  256 48 224 224\n",
      "scaled:  256 48 224 224\n",
      "org:  272 48 224 224\n",
      "scaled:  272 48 224 224\n",
      "org:  288 48 224 224\n",
      "scaled:  288 48 224 224\n",
      "org:  304 48 224 224\n",
      "scaled:  304 48 224 224\n",
      "org:  320 48 224 224\n",
      "scaled:  320 48 224 224\n",
      "org:  336 48 224 224\n",
      "scaled:  336 48 224 224\n",
      "org:  352 48 224 224\n",
      "scaled:  352 48 224 224\n",
      "org:  368 48 224 224\n",
      "scaled:  368 48 224 224\n",
      "org:  0 64 224 224\n",
      "scaled:  0 64 224 224\n",
      "org:  16 64 224 224\n",
      "scaled:  16 64 224 224\n",
      "org:  32 64 224 224\n",
      "scaled:  32 64 224 224\n",
      "org:  48 64 224 224\n",
      "scaled:  48 64 224 224\n",
      "org:  64 64 224 224\n",
      "scaled:  64 64 224 224\n",
      "org:  80 64 224 224\n",
      "scaled:  80 64 224 224\n",
      "org:  96 64 224 224\n",
      "scaled:  96 64 224 224\n",
      "org:  112 64 224 224\n",
      "scaled:  112 64 224 224\n",
      "org:  128 64 224 224\n",
      "scaled:  128 64 224 224\n",
      "org:  144 64 224 224\n",
      "scaled:  144 64 224 224\n",
      "org:  160 64 224 224\n",
      "scaled:  160 64 224 224\n",
      "org:  176 64 224 224\n",
      "scaled:  176 64 224 224\n",
      "org:  192 64 224 224\n",
      "scaled:  192 64 224 224\n",
      "org:  208 64 224 224\n",
      "scaled:  208 64 224 224\n",
      "org:  224 64 224 224\n",
      "scaled:  224 64 224 224\n",
      "org:  240 64 224 224\n",
      "scaled:  240 64 224 224\n",
      "org:  256 64 224 224\n",
      "scaled:  256 64 224 224\n",
      "org:  272 64 224 224\n",
      "scaled:  272 64 224 224\n",
      "org:  288 64 224 224\n",
      "scaled:  288 64 224 224\n",
      "org:  304 64 224 224\n",
      "scaled:  304 64 224 224\n",
      "org:  320 64 224 224\n",
      "scaled:  320 64 224 224\n",
      "org:  336 64 224 224\n",
      "scaled:  336 64 224 224\n",
      "org:  352 64 224 224\n",
      "scaled:  352 64 224 224\n",
      "org:  368 64 224 224\n",
      "scaled:  368 64 224 224\n",
      "org:  0 80 224 224\n",
      "scaled:  0 80 224 224\n",
      "org:  16 80 224 224\n",
      "scaled:  16 80 224 224\n",
      "org:  32 80 224 224\n",
      "scaled:  32 80 224 224\n",
      "org:  48 80 224 224\n",
      "scaled:  48 80 224 224\n",
      "org:  64 80 224 224\n",
      "scaled:  64 80 224 224\n",
      "org:  80 80 224 224\n",
      "scaled:  80 80 224 224\n",
      "org:  96 80 224 224\n",
      "scaled:  96 80 224 224\n",
      "org:  112 80 224 224\n",
      "scaled:  112 80 224 224\n",
      "org:  128 80 224 224\n",
      "scaled:  128 80 224 224\n",
      "org:  144 80 224 224\n",
      "scaled:  144 80 224 224\n",
      "org:  160 80 224 224\n",
      "scaled:  160 80 224 224\n",
      "org:  176 80 224 224\n",
      "scaled:  176 80 224 224\n",
      "org:  192 80 224 224\n",
      "scaled:  192 80 224 224\n",
      "org:  208 80 224 224\n",
      "scaled:  208 80 224 224\n",
      "org:  224 80 224 224\n",
      "scaled:  224 80 224 224\n",
      "org:  240 80 224 224\n",
      "scaled:  240 80 224 224\n",
      "org:  256 80 224 224\n",
      "scaled:  256 80 224 224\n",
      "org:  272 80 224 224\n",
      "scaled:  272 80 224 224\n",
      "org:  288 80 224 224\n",
      "scaled:  288 80 224 224\n",
      "org:  304 80 224 224\n",
      "scaled:  304 80 224 224\n",
      "org:  320 80 224 224\n",
      "scaled:  320 80 224 224\n",
      "org:  336 80 224 224\n",
      "scaled:  336 80 224 224\n",
      "org:  352 80 224 224\n",
      "scaled:  352 80 224 224\n",
      "org:  368 80 224 224\n",
      "scaled:  368 80 224 224\n",
      "org:  0 96 224 224\n",
      "scaled:  0 96 224 224\n",
      "org:  16 96 224 224\n",
      "scaled:  16 96 224 224\n",
      "org:  32 96 224 224\n",
      "scaled:  32 96 224 224\n",
      "org:  48 96 224 224\n",
      "scaled:  48 96 224 224\n",
      "org:  64 96 224 224\n",
      "scaled:  64 96 224 224\n",
      "org:  80 96 224 224\n",
      "scaled:  80 96 224 224\n",
      "org:  96 96 224 224\n",
      "scaled:  96 96 224 224\n",
      "org:  112 96 224 224\n",
      "scaled:  112 96 224 224\n",
      "org:  128 96 224 224\n",
      "scaled:  128 96 224 224\n",
      "org:  144 96 224 224\n",
      "scaled:  144 96 224 224\n",
      "org:  160 96 224 224\n",
      "scaled:  160 96 224 224\n",
      "org:  176 96 224 224\n",
      "scaled:  176 96 224 224\n",
      "org:  192 96 224 224\n",
      "scaled:  192 96 224 224\n",
      "org:  208 96 224 224\n",
      "scaled:  208 96 224 224\n",
      "org:  224 96 224 224\n",
      "scaled:  224 96 224 224\n",
      "org:  240 96 224 224\n",
      "scaled:  240 96 224 224\n",
      "org:  256 96 224 224\n",
      "scaled:  256 96 224 224\n",
      "org:  272 96 224 224\n",
      "scaled:  272 96 224 224\n",
      "org:  288 96 224 224\n",
      "scaled:  288 96 224 224\n",
      "org:  304 96 224 224\n",
      "scaled:  304 96 224 224\n",
      "org:  320 96 224 224\n",
      "scaled:  320 96 224 224\n",
      "org:  336 96 224 224\n",
      "scaled:  336 96 224 224\n",
      "org:  352 96 224 224\n",
      "scaled:  352 96 224 224\n",
      "org:  368 96 224 224\n",
      "scaled:  368 96 224 224\n",
      "org:  0 112 224 224\n",
      "scaled:  0 112 224 224\n",
      "org:  16 112 224 224\n",
      "scaled:  16 112 224 224\n",
      "org:  32 112 224 224\n",
      "scaled:  32 112 224 224\n",
      "org:  48 112 224 224\n",
      "scaled:  48 112 224 224\n",
      "org:  64 112 224 224\n",
      "scaled:  64 112 224 224\n",
      "org:  80 112 224 224\n",
      "scaled:  80 112 224 224\n",
      "org:  96 112 224 224\n",
      "scaled:  96 112 224 224\n",
      "org:  112 112 224 224\n",
      "scaled:  112 112 224 224\n",
      "org:  128 112 224 224\n",
      "scaled:  128 112 224 224\n",
      "org:  144 112 224 224\n",
      "scaled:  144 112 224 224\n",
      "org:  160 112 224 224\n",
      "scaled:  160 112 224 224\n",
      "org:  176 112 224 224\n",
      "scaled:  176 112 224 224\n",
      "org:  192 112 224 224\n",
      "scaled:  192 112 224 224\n",
      "org:  208 112 224 224\n",
      "scaled:  208 112 224 224\n",
      "org:  224 112 224 224\n",
      "scaled:  224 112 224 224\n",
      "org:  240 112 224 224\n",
      "scaled:  240 112 224 224\n",
      "org:  256 112 224 224\n",
      "scaled:  256 112 224 224\n",
      "org:  272 112 224 224\n",
      "scaled:  272 112 224 224\n",
      "org:  288 112 224 224\n",
      "scaled:  288 112 224 224\n",
      "org:  304 112 224 224\n",
      "scaled:  304 112 224 224\n",
      "org:  320 112 224 224\n",
      "scaled:  320 112 224 224\n",
      "org:  336 112 224 224\n",
      "scaled:  336 112 224 224\n",
      "org:  352 112 224 224\n",
      "scaled:  352 112 224 224\n",
      "org:  368 112 224 224\n",
      "scaled:  368 112 224 224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org:  0 128 224 224\n",
      "scaled:  0 128 224 224\n",
      "org:  16 128 224 224\n",
      "scaled:  16 128 224 224\n",
      "org:  32 128 224 224\n",
      "scaled:  32 128 224 224\n",
      "org:  48 128 224 224\n",
      "scaled:  48 128 224 224\n",
      "org:  64 128 224 224\n",
      "scaled:  64 128 224 224\n",
      "org:  80 128 224 224\n",
      "scaled:  80 128 224 224\n",
      "org:  96 128 224 224\n",
      "scaled:  96 128 224 224\n",
      "org:  112 128 224 224\n",
      "scaled:  112 128 224 224\n",
      "org:  128 128 224 224\n",
      "scaled:  128 128 224 224\n",
      "org:  144 128 224 224\n",
      "scaled:  144 128 224 224\n",
      "org:  160 128 224 224\n",
      "scaled:  160 128 224 224\n",
      "org:  176 128 224 224\n",
      "scaled:  176 128 224 224\n",
      "org:  192 128 224 224\n",
      "scaled:  192 128 224 224\n",
      "org:  208 128 224 224\n",
      "scaled:  208 128 224 224\n",
      "org:  224 128 224 224\n",
      "scaled:  224 128 224 224\n",
      "org:  240 128 224 224\n",
      "scaled:  240 128 224 224\n",
      "org:  256 128 224 224\n",
      "scaled:  256 128 224 224\n",
      "org:  272 128 224 224\n",
      "scaled:  272 128 224 224\n",
      "org:  288 128 224 224\n",
      "scaled:  288 128 224 224\n",
      "org:  304 128 224 224\n",
      "scaled:  304 128 224 224\n",
      "org:  320 128 224 224\n",
      "scaled:  320 128 224 224\n",
      "org:  336 128 224 224\n",
      "scaled:  336 128 224 224\n",
      "org:  352 128 224 224\n",
      "scaled:  352 128 224 224\n",
      "org:  368 128 224 224\n",
      "scaled:  368 128 224 224\n",
      "org:  0 144 224 224\n",
      "scaled:  0 144 224 224\n",
      "org:  16 144 224 224\n",
      "scaled:  16 144 224 224\n",
      "org:  32 144 224 224\n",
      "scaled:  32 144 224 224\n",
      "org:  48 144 224 224\n",
      "scaled:  48 144 224 224\n",
      "org:  64 144 224 224\n",
      "scaled:  64 144 224 224\n",
      "org:  80 144 224 224\n",
      "scaled:  80 144 224 224\n",
      "org:  96 144 224 224\n",
      "scaled:  96 144 224 224\n",
      "org:  112 144 224 224\n",
      "scaled:  112 144 224 224\n",
      "org:  128 144 224 224\n",
      "scaled:  128 144 224 224\n",
      "org:  144 144 224 224\n",
      "scaled:  144 144 224 224\n",
      "org:  160 144 224 224\n",
      "scaled:  160 144 224 224\n",
      "org:  176 144 224 224\n",
      "scaled:  176 144 224 224\n",
      "org:  192 144 224 224\n",
      "scaled:  192 144 224 224\n",
      "org:  208 144 224 224\n",
      "scaled:  208 144 224 224\n",
      "org:  224 144 224 224\n",
      "scaled:  224 144 224 224\n",
      "org:  240 144 224 224\n",
      "scaled:  240 144 224 224\n",
      "org:  256 144 224 224\n",
      "scaled:  256 144 224 224\n",
      "org:  272 144 224 224\n",
      "scaled:  272 144 224 224\n",
      "org:  288 144 224 224\n",
      "scaled:  288 144 224 224\n",
      "org:  304 144 224 224\n",
      "scaled:  304 144 224 224\n",
      "org:  320 144 224 224\n",
      "scaled:  320 144 224 224\n",
      "org:  336 144 224 224\n",
      "scaled:  336 144 224 224\n",
      "org:  352 144 224 224\n",
      "scaled:  352 144 224 224\n",
      "org:  368 144 224 224\n",
      "scaled:  368 144 224 224\n",
      "org:  0 160 224 224\n",
      "scaled:  0 160 224 224\n",
      "org:  16 160 224 224\n",
      "scaled:  16 160 224 224\n",
      "org:  32 160 224 224\n",
      "scaled:  32 160 224 224\n",
      "org:  48 160 224 224\n",
      "scaled:  48 160 224 224\n",
      "org:  64 160 224 224\n",
      "scaled:  64 160 224 224\n",
      "org:  80 160 224 224\n",
      "scaled:  80 160 224 224\n",
      "org:  96 160 224 224\n",
      "scaled:  96 160 224 224\n",
      "org:  112 160 224 224\n",
      "scaled:  112 160 224 224\n",
      "org:  128 160 224 224\n",
      "scaled:  128 160 224 224\n",
      "org:  144 160 224 224\n",
      "scaled:  144 160 224 224\n",
      "org:  160 160 224 224\n",
      "scaled:  160 160 224 224\n",
      "org:  176 160 224 224\n",
      "scaled:  176 160 224 224\n",
      "org:  192 160 224 224\n",
      "scaled:  192 160 224 224\n",
      "org:  208 160 224 224\n",
      "scaled:  208 160 224 224\n",
      "org:  224 160 224 224\n",
      "scaled:  224 160 224 224\n",
      "org:  240 160 224 224\n",
      "scaled:  240 160 224 224\n",
      "org:  256 160 224 224\n",
      "scaled:  256 160 224 224\n",
      "org:  272 160 224 224\n",
      "scaled:  272 160 224 224\n",
      "org:  288 160 224 224\n",
      "scaled:  288 160 224 224\n",
      "org:  304 160 224 224\n",
      "scaled:  304 160 224 224\n",
      "org:  320 160 224 224\n",
      "scaled:  320 160 224 224\n",
      "org:  336 160 224 224\n",
      "scaled:  336 160 224 224\n",
      "org:  352 160 224 224\n",
      "scaled:  352 160 224 224\n",
      "org:  368 160 224 224\n",
      "scaled:  368 160 224 224\n",
      "org:  0 176 224 224\n",
      "scaled:  0 176 224 224\n",
      "org:  16 176 224 224\n",
      "scaled:  16 176 224 224\n",
      "org:  32 176 224 224\n",
      "scaled:  32 176 224 224\n",
      "org:  48 176 224 224\n",
      "scaled:  48 176 224 224\n",
      "org:  64 176 224 224\n",
      "scaled:  64 176 224 224\n",
      "org:  80 176 224 224\n",
      "scaled:  80 176 224 224\n",
      "org:  96 176 224 224\n",
      "scaled:  96 176 224 224\n",
      "org:  112 176 224 224\n",
      "scaled:  112 176 224 224\n",
      "org:  128 176 224 224\n",
      "scaled:  128 176 224 224\n",
      "org:  144 176 224 224\n",
      "scaled:  144 176 224 224\n",
      "org:  160 176 224 224\n",
      "scaled:  160 176 224 224\n",
      "org:  176 176 224 224\n",
      "scaled:  176 176 224 224\n",
      "org:  192 176 224 224\n",
      "scaled:  192 176 224 224\n",
      "org:  208 176 224 224\n",
      "scaled:  208 176 224 224\n",
      "org:  224 176 224 224\n",
      "scaled:  224 176 224 224\n",
      "org:  240 176 224 224\n",
      "scaled:  240 176 224 224\n",
      "org:  256 176 224 224\n",
      "scaled:  256 176 224 224\n",
      "org:  272 176 224 224\n",
      "scaled:  272 176 224 224\n",
      "org:  288 176 224 224\n",
      "scaled:  288 176 224 224\n",
      "org:  304 176 224 224\n",
      "scaled:  304 176 224 224\n",
      "org:  320 176 224 224\n",
      "scaled:  320 176 224 224\n",
      "org:  336 176 224 224\n",
      "scaled:  336 176 224 224\n",
      "org:  352 176 224 224\n",
      "scaled:  352 176 224 224\n",
      "org:  368 176 224 224\n",
      "scaled:  368 176 224 224\n",
      "org:  0 192 224 224\n",
      "scaled:  0 192 224 224\n",
      "org:  16 192 224 224\n",
      "scaled:  16 192 224 224\n",
      "org:  32 192 224 224\n",
      "scaled:  32 192 224 224\n",
      "org:  48 192 224 224\n",
      "scaled:  48 192 224 224\n",
      "org:  64 192 224 224\n",
      "scaled:  64 192 224 224\n",
      "org:  80 192 224 224\n",
      "scaled:  80 192 224 224\n",
      "org:  96 192 224 224\n",
      "scaled:  96 192 224 224\n",
      "org:  112 192 224 224\n",
      "scaled:  112 192 224 224\n",
      "org:  128 192 224 224\n",
      "scaled:  128 192 224 224\n",
      "org:  144 192 224 224\n",
      "scaled:  144 192 224 224\n",
      "org:  160 192 224 224\n",
      "scaled:  160 192 224 224\n",
      "org:  176 192 224 224\n",
      "scaled:  176 192 224 224\n",
      "org:  192 192 224 224\n",
      "scaled:  192 192 224 224\n",
      "org:  208 192 224 224\n",
      "scaled:  208 192 224 224\n",
      "org:  224 192 224 224\n",
      "scaled:  224 192 224 224\n",
      "org:  240 192 224 224\n",
      "scaled:  240 192 224 224\n",
      "org:  256 192 224 224\n",
      "scaled:  256 192 224 224\n",
      "org:  272 192 224 224\n",
      "scaled:  272 192 224 224\n",
      "org:  288 192 224 224\n",
      "scaled:  288 192 224 224\n",
      "org:  304 192 224 224\n",
      "scaled:  304 192 224 224\n",
      "org:  320 192 224 224\n",
      "scaled:  320 192 224 224\n",
      "org:  336 192 224 224\n",
      "scaled:  336 192 224 224\n",
      "org:  352 192 224 224\n",
      "scaled:  352 192 224 224\n",
      "org:  368 192 224 224\n",
      "scaled:  368 192 224 224\n",
      "org:  0 208 224 224\n",
      "scaled:  0 208 224 224\n",
      "org:  16 208 224 224\n",
      "scaled:  16 208 224 224\n",
      "org:  32 208 224 224\n",
      "scaled:  32 208 224 224\n",
      "org:  48 208 224 224\n",
      "scaled:  48 208 224 224\n",
      "org:  64 208 224 224\n",
      "scaled:  64 208 224 224\n",
      "org:  80 208 224 224\n",
      "scaled:  80 208 224 224\n",
      "org:  96 208 224 224\n",
      "scaled:  96 208 224 224\n",
      "org:  112 208 224 224\n",
      "scaled:  112 208 224 224\n",
      "org:  128 208 224 224\n",
      "scaled:  128 208 224 224\n",
      "org:  144 208 224 224\n",
      "scaled:  144 208 224 224\n",
      "org:  160 208 224 224\n",
      "scaled:  160 208 224 224\n",
      "org:  176 208 224 224\n",
      "scaled:  176 208 224 224\n",
      "org:  192 208 224 224\n",
      "scaled:  192 208 224 224\n",
      "org:  208 208 224 224\n",
      "scaled:  208 208 224 224\n",
      "org:  224 208 224 224\n",
      "scaled:  224 208 224 224\n",
      "org:  240 208 224 224\n",
      "scaled:  240 208 224 224\n",
      "org:  256 208 224 224\n",
      "scaled:  256 208 224 224\n",
      "org:  272 208 224 224\n",
      "scaled:  272 208 224 224\n",
      "org:  288 208 224 224\n",
      "scaled:  288 208 224 224\n",
      "org:  304 208 224 224\n",
      "scaled:  304 208 224 224\n",
      "org:  320 208 224 224\n",
      "scaled:  320 208 224 224\n",
      "org:  336 208 224 224\n",
      "scaled:  336 208 224 224\n",
      "org:  352 208 224 224\n",
      "scaled:  352 208 224 224\n",
      "org:  368 208 224 224\n",
      "scaled:  368 208 224 224\n",
      "org:  0 224 224 224\n",
      "scaled:  0 224 224 224\n",
      "org:  16 224 224 224\n",
      "scaled:  16 224 224 224\n",
      "org:  32 224 224 224\n",
      "scaled:  32 224 224 224\n",
      "org:  48 224 224 224\n",
      "scaled:  48 224 224 224\n",
      "org:  64 224 224 224\n",
      "scaled:  64 224 224 224\n",
      "org:  80 224 224 224\n",
      "scaled:  80 224 224 224\n",
      "org:  96 224 224 224\n",
      "scaled:  96 224 224 224\n",
      "org:  112 224 224 224\n",
      "scaled:  112 224 224 224\n",
      "org:  128 224 224 224\n",
      "scaled:  128 224 224 224\n",
      "org:  144 224 224 224\n",
      "scaled:  144 224 224 224\n",
      "org:  160 224 224 224\n",
      "scaled:  160 224 224 224\n",
      "org:  176 224 224 224\n",
      "scaled:  176 224 224 224\n",
      "org:  192 224 224 224\n",
      "scaled:  192 224 224 224\n",
      "org:  208 224 224 224\n",
      "scaled:  208 224 224 224\n",
      "org:  224 224 224 224\n",
      "scaled:  224 224 224 224\n",
      "org:  240 224 224 224\n",
      "scaled:  240 224 224 224\n",
      "org:  256 224 224 224\n",
      "scaled:  256 224 224 224\n",
      "org:  272 224 224 224\n",
      "scaled:  272 224 224 224\n",
      "org:  288 224 224 224\n",
      "scaled:  288 224 224 224\n",
      "org:  304 224 224 224\n",
      "scaled:  304 224 224 224\n",
      "org:  320 224 224 224\n",
      "scaled:  320 224 224 224\n",
      "org:  336 224 224 224\n",
      "scaled:  336 224 224 224\n",
      "org:  352 224 224 224\n",
      "scaled:  352 224 224 224\n",
      "org:  368 224 224 224\n",
      "scaled:  368 224 224 224\n",
      "org:  0 0 224 224\n",
      "scaled:  0 0 336 336\n",
      "org:  16 0 224 224\n",
      "scaled:  24 0 336 336\n",
      "org:  32 0 224 224\n",
      "scaled:  48 0 336 336\n",
      "org:  48 0 224 224\n",
      "scaled:  72 0 336 336\n",
      "org:  64 0 224 224\n",
      "scaled:  96 0 336 336\n",
      "org:  80 0 224 224\n",
      "scaled:  120 0 336 336\n",
      "org:  96 0 224 224\n",
      "scaled:  144 0 336 336\n",
      "org:  112 0 224 224\n",
      "scaled:  168 0 336 336\n",
      "org:  128 0 224 224\n",
      "scaled:  192 0 336 336\n",
      "org:  144 0 224 224\n",
      "scaled:  216 0 336 336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org:  160 0 224 224\n",
      "scaled:  240 0 336 336\n",
      "org:  0 16 224 224\n",
      "scaled:  0 24 336 336\n",
      "org:  16 16 224 224\n",
      "scaled:  24 24 336 336\n",
      "org:  32 16 224 224\n",
      "scaled:  48 24 336 336\n",
      "org:  48 16 224 224\n",
      "scaled:  72 24 336 336\n",
      "org:  64 16 224 224\n",
      "scaled:  96 24 336 336\n",
      "org:  80 16 224 224\n",
      "scaled:  120 24 336 336\n",
      "org:  96 16 224 224\n",
      "scaled:  144 24 336 336\n",
      "org:  112 16 224 224\n",
      "scaled:  168 24 336 336\n",
      "org:  128 16 224 224\n",
      "scaled:  192 24 336 336\n",
      "org:  144 16 224 224\n",
      "scaled:  216 24 336 336\n",
      "org:  160 16 224 224\n",
      "scaled:  240 24 336 336\n",
      "org:  0 32 224 224\n",
      "scaled:  0 48 336 336\n",
      "org:  16 32 224 224\n",
      "scaled:  24 48 336 336\n",
      "org:  32 32 224 224\n",
      "scaled:  48 48 336 336\n",
      "org:  48 32 224 224\n",
      "scaled:  72 48 336 336\n",
      "org:  64 32 224 224\n",
      "scaled:  96 48 336 336\n",
      "org:  80 32 224 224\n",
      "scaled:  120 48 336 336\n",
      "org:  96 32 224 224\n",
      "scaled:  144 48 336 336\n",
      "org:  112 32 224 224\n",
      "scaled:  168 48 336 336\n",
      "org:  128 32 224 224\n",
      "scaled:  192 48 336 336\n",
      "org:  144 32 224 224\n",
      "scaled:  216 48 336 336\n",
      "org:  160 32 224 224\n",
      "scaled:  240 48 336 336\n",
      "org:  0 48 224 224\n",
      "scaled:  0 72 336 336\n",
      "org:  16 48 224 224\n",
      "scaled:  24 72 336 336\n",
      "org:  32 48 224 224\n",
      "scaled:  48 72 336 336\n",
      "org:  48 48 224 224\n",
      "scaled:  72 72 336 336\n",
      "org:  64 48 224 224\n",
      "scaled:  96 72 336 336\n",
      "org:  80 48 224 224\n",
      "scaled:  120 72 336 336\n",
      "org:  96 48 224 224\n",
      "scaled:  144 72 336 336\n",
      "org:  112 48 224 224\n",
      "scaled:  168 72 336 336\n",
      "org:  128 48 224 224\n",
      "scaled:  192 72 336 336\n",
      "org:  144 48 224 224\n",
      "scaled:  216 72 336 336\n",
      "org:  160 48 224 224\n",
      "scaled:  240 72 336 336\n",
      "org:  0 64 224 224\n",
      "scaled:  0 96 336 336\n",
      "org:  16 64 224 224\n",
      "scaled:  24 96 336 336\n",
      "org:  32 64 224 224\n",
      "scaled:  48 96 336 336\n",
      "org:  48 64 224 224\n",
      "scaled:  72 96 336 336\n",
      "org:  64 64 224 224\n",
      "scaled:  96 96 336 336\n",
      "org:  80 64 224 224\n",
      "scaled:  120 96 336 336\n",
      "org:  96 64 224 224\n",
      "scaled:  144 96 336 336\n",
      "org:  112 64 224 224\n",
      "scaled:  168 96 336 336\n",
      "org:  128 64 224 224\n",
      "scaled:  192 96 336 336\n",
      "org:  144 64 224 224\n",
      "scaled:  216 96 336 336\n",
      "org:  160 64 224 224\n",
      "scaled:  240 96 336 336\n"
     ]
    }
   ],
   "source": [
    "# initialize the image pyramid\n",
    "pyramid = image_pyramid(img, scale=pyr_scale, minSize=roi_size)\n",
    "# initialize two lists, one to hold the ROIs generated from the image\n",
    "# pyramid and sliding window, and another list used to store the\n",
    "# (x, y)-coordinates of where the ROI was in the original image\n",
    "rois = []\n",
    "locs = []\n",
    "# time how long it takes to loop over the image pyramid layers and\n",
    "# sliding window locations\n",
    "start = time.time()\n",
    "# loop over the image pyramid\n",
    "for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "    scale = img_w / float(image.shape[1])\n",
    "    # for each layer of the image pyramid, loop over the sliding\n",
    "    # window locations\n",
    "    for (x, y, roi_cropped) in sliding_window(image, step, roi_size):\n",
    "        # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "        # *original* image dimensions\n",
    "        org_x,org_y,org_w,org_h=x,y,roi_size[0],roi_size[1]\n",
    "        print(\"org: \",org_x,org_y,org_w,org_h)\n",
    "        x = int(x * scale)\n",
    "        y = int(y * scale)\n",
    "        w = int(roi_size[0] * scale)\n",
    "        h = int(roi_size[1] * scale)\n",
    "        print(\"scaled: \", x,y,w,h)\n",
    "        # take the ROI and preprocess it so we can later classify\n",
    "        # the region using Keras/TensorFlow\n",
    "        roi = cv2.resize(roi_cropped, model_input_size)\n",
    "        roi = img_to_array(roi)\n",
    "        roi = preprocess_input(roi)\n",
    "        # update our list of ROIs and associated coordinates\n",
    "        rois.append(roi)\n",
    "        locs.append((x, y, x + w, y + h))\n",
    "        \n",
    "        img_vis=image.copy()\n",
    "        img_roi=roi_cropped.copy()\n",
    "        cv2.rectangle(img_vis, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "        cv2.imshow(\"img\", img_vis)\n",
    "        cv2.imshow(\"cropped_roi\", img_roi)\n",
    "        cv2.waitKey(30)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9fb4a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blue scaled\n",
    "# green original\n",
    "img_test=image.copy()\n",
    "cv2.rectangle(img_test, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "cv2.rectangle(img_test, (org_x, org_y), (org_x+org_w, org_y+org_h), (0,255,0), 2)\n",
    "\n",
    "img_test_org=img.copy()\n",
    "cv2.rectangle(img_test_org, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "cv2.rectangle(img_test_org, (org_x, org_y), (org_x+org_w, org_y+org_h), (0,255,0), 2)\n",
    "\n",
    "cv2.imshow(\"img_resized\", img_test)\n",
    "cv2.imshow(\"img_original\", img_test_org)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db19709c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_test_org=img.copy()\n",
    "cv2.rectangle(img_test_org, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "\n",
    "cv2.rectangle(img_test_org, (org_x, org_y), (org_x+org_w, org_y+org_h), (0,255,0), 2)\n",
    "cv2.imshow(\"img2\", img_test_org)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dce56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop with the original coord from resized image\n",
    "# crop with the scaled coord from original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e140d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_resized_crop=image[org_y:org_y + org_w, org_x:org_x + org_h]\n",
    "img_org_crop=img[y:y + w, x:x + h]\n",
    "img_org_crop_org_coord=img[org_y:org_y + org_w, org_x:org_x + org_h]\n",
    "cv2.imshow(\"img_resized_crop\", img_resized_crop)\n",
    "cv2.imshow(\"img_org_crop\", img_org_crop)\n",
    "cv2.imshow(\"img_org_crop_org_coord\", img_org_crop_org_coord)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3e877",
   "metadata": {},
   "source": [
    "### Drawbacks of the above approach\n",
    "### 1. It is a very slow approcah to slide windows and if we add image pyramid, it further slows. \n",
    "### 2. Very tough to estimate the window size and stride, This however determines the accuracy of the object detector.\n",
    "### 3. This is not end to end trainable, whihc means we cannot correct the error on the boundin box location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17fe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
