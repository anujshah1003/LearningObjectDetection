{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce95e24",
   "metadata": {},
   "source": [
    "## Converting classifier to detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c28c4c",
   "metadata": {},
   "source": [
    "### Learning and code taken from Adrian's Tutorial on Object detcetion :\n",
    "https://pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de2a0f",
   "metadata": {},
   "source": [
    "### 1. Using sliding window \n",
    "####  a.Use sliding window to get the ROI (location) and feed that into your CNN classifier\n",
    "####  b. No change ine the deep neural network\n",
    "####   c. Need to implement a preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de12f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a19fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0] - ws[1], step):\n",
    "        for x in range(0, image.shape[1] - ws[0], step):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + ws[1], x:x + ws[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b80a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step : stride for sliding the window across image (recommendation is 4 to 8 pixels)\n",
    "# ws: The ROI or window size for cropping out the image and feeding to the CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79f1065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 479, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path=r\"D:\\LearningObjectDetection\\images\"\n",
    "image_path = os.path.join(base_path,\"dog1.jpg\")\n",
    "img = cv2.imread(image_path)\n",
    "print(img.shape)\n",
    "cv2.imshow(\"img\", img)\n",
    "cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a923e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step=16\n",
    "window_size=(224,224)\n",
    "img_resize_width=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables used for the object detection procedure\n",
    "WIDTH = 600\n",
    "PYR_SCALE = 1.5\n",
    "WIN_STEP = 16\n",
    "#ROI_SIZE = eval(args[\"size\"])\n",
    "INPUT_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y,img_crop in sliding_window(img, step, window_size):\n",
    "    print(x,y)\n",
    "    img_vis=img.copy()\n",
    "    cv2.rectangle(img_vis, (x, y), (x+window_size[0], y+window_size[0]), (255,0,0), 2)\n",
    "    cv2.imshow(\"img\", img_vis)\n",
    "    cv2.waitKey(30)\n",
    "cv2.destroAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing the resnet model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c64249",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_size=(224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] loading network...\")\n",
    "model = ResNet50(weights=\"imagenet\", include_top=True)\n",
    "# load the input image from disk, resize it such that it has the\n",
    "# has the supplied width, and then grab its dimensions\n",
    "img = cv2.imread(image_path)\n",
    "print(\"[INFO] image shape: \",img.shape)\n",
    "img = cv2.resize(img, model_input_size)\n",
    "#img = imutils.resize(img, width=img_resize_width)\n",
    "print(\"[INFO] image shape after resize: \",img.shape)\n",
    "img = img_to_array(img)\n",
    "img = preprocess_input(img)\n",
    "img = np.expand_dims(img,axis=0)\n",
    "print(\"[INFO] image shape after preprocessing: \",img.shape)\n",
    "\n",
    "\n",
    "#(H, W) = img.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de487b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] classifying the image...\")\n",
    "start = time.time()\n",
    "preds = model.predict(img)\n",
    "end = time.time()\n",
    "print(\"[INFO] classifying image took {:.5f} seconds\".format(end - start))\n",
    "print(\"[INFO] Prediction values: \")\n",
    "print(\"num prediction: \",len(preds[0]))\n",
    "#print(preds)\n",
    "print(\"max value and class number: \",np.max(preds[0]),\"and\",np.argmax(preds[0]))\n",
    "# decode the predictions and initialize a dictionary which maps class\n",
    "# labels (keys) to any ROIs associated with that label (values)\n",
    "preds = imagenet_utils.decode_predictions(preds, top=1)\n",
    "print (\"prediction decoded: Imagenet ID, Class Name, Class Prob\")\n",
    "print(preds)\n",
    "labels = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea37a46",
   "metadata": {},
   "source": [
    "### Using Sliding window and ResNet50 for object detection\n",
    "### Many of the ROIs will be simply background so wee need to filter out those based on threshold probbility score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee465880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_conf=0.6\n",
    "min_conf=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the sliding window locations and store each region and its location\n",
    "rois=[]\n",
    "locs=[]\n",
    "img = cv2.imread(image_path)\n",
    "for (x, y, roi_cropped) in sliding_window(img,  step, window_size):\n",
    "\n",
    "    w=window_size[0]\n",
    "    h=window_size[1]\n",
    "    # take the ROI and preprocess it so we can later classify\n",
    "    # the region using Keras/TensorFlow\n",
    "    roi = cv2.resize(roi_cropped, model_input_size)\n",
    "    roi = img_to_array(roi)\n",
    "    roi = preprocess_input(roi)\n",
    "    # update our list of ROIs and associated coordinates\n",
    "    rois.append(roi)\n",
    "    locs.append((x, y, x+w, y+h))\n",
    "    \n",
    "    img_vis=img.copy()\n",
    "    img_roi=roi_cropped.copy()\n",
    "    cv2.rectangle(img_vis, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "    cv2.imshow(\"img\", img_vis)\n",
    "    cv2.imshow(\"cropped_roi\", img_roi)\n",
    "    cv2.waitKey(30)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583817a6",
   "metadata": {},
   "source": [
    "#### Do the prediction on all rois as a batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de376ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the ROIs to a NumPy array\n",
    "rois = np.array(rois, dtype=\"float32\")\n",
    "# classify each of the proposal ROIs using ResNet and then show how\n",
    "# long the classifications took\n",
    "print(\"[INFO] classifying ROIs...\")\n",
    "start = time.time()\n",
    "preds = model.predict(rois)\n",
    "end = time.time()\n",
    "print(\"[INFO] classifying {} ROIs took {:.5f} seconds\".format(rois.shape[0],end - start))\n",
    "# decode the predictions and initialize a dictionary which maps class\n",
    "# labels (keys) to any ROIs associated with that label (values)\n",
    "preds = imagenet_utils.decode_predictions(preds, top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "# loop over the predictions\n",
    "for (i, p) in enumerate(preds):\n",
    "\t# grab the prediction information for the current ROI\n",
    "\t(imagenetID, label, prob) = p[0]\n",
    "\t# filter out weak detections by ensuring the predicted probability\n",
    "\t# is greater than the minimum probability\n",
    "\tif prob >= min_conf:\n",
    "\t\t# grab the bounding box associated with the prediction and\n",
    "\t\t# convert the coordinates\n",
    "\t\tbox = locs[i]\n",
    "\t\t# grab the list of predictions for the label and add the\n",
    "\t\t# bounding box and probability to the list\n",
    "\t\tL = labels.get(label, [])\n",
    "\t\tL.append((box, prob))\n",
    "\t\tlabels[label] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label)\n",
    "L = labels.get(label, [])\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the labels for each of detected objects in the image\n",
    "img_all_preds = img.copy()\n",
    "for label in labels.keys():\n",
    "    # clone the original image so that we can draw on it\n",
    "    print(\"[INFO] showing results for '{}'\".format(label))\n",
    "    clone = img.copy()\n",
    "    # loop over all bounding boxes for the current label\n",
    "    for (box, prob) in labels[label]:\n",
    "        # draw the bounding box on the image\n",
    "        (startX, startY, endX, endY) = box\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "        cv2.rectangle(img_all_preds, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "    cv2.imshow(\"{}\".format(label), clone)\n",
    "    #cv2.waitKey(0)\n",
    "    #clone = orig.copy()\n",
    "cv2.imshow(\"Before\", img_all_preds)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying NMS to get rid of boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the labels for each of detected objects in the image\n",
    "img_all_preds_bf=img.copy()\n",
    "img_all_preds_af=img.copy()\n",
    "\n",
    "for label in labels.keys():\n",
    "    # clone the original image so that we can draw on it\n",
    "    print(\"[INFO] showing results for '{}'\".format(label))\n",
    "    clone = img.copy()\n",
    "    # loop over all bounding boxes for the current label\n",
    "    for (box, prob) in labels[label]:\n",
    "        # draw the bounding box on the image\n",
    "        (startX, startY, endX, endY) = box\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "        cv2.rectangle(img_all_preds_bf, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "    cv2.imshow(\"{} Before\".format(label), clone)\n",
    "    \n",
    "    # extract the bounding boxes and associated prediction robabilities, then apply non-maxima suppression\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, proba,overlapThresh=0.2)\n",
    "    # loop over all bounding boxes that were kept after applying\n",
    "    # non-maxima suppression\n",
    "    clone = img.copy()\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # draw the bounding box and label on the image\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        cv2.putText(clone, str(endX-startx), (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.rectangle(img_all_preds_af, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "    # show the output after apply non-maxima suppression\n",
    "    cv2.imshow(\"{} After\".format(label), clone)\n",
    "\n",
    "    #cv2.waitKey(0)\n",
    "    #clone = orig.copy()\n",
    "cv2.imshow(\"Before NMS\", img_all_preds_bf)\n",
    "cv2.imshow(\"After NMS\", img_all_preds_af)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72efb90",
   "metadata": {},
   "source": [
    "#### we see false positives here whihc we can reduce by increasing the min confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06ab7a",
   "metadata": {},
   "source": [
    "## Image Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e294bb6",
   "metadata": {},
   "source": [
    "#### Utilizing an image pyramid allows us to find objects in images at different scales (sizes) of an image. And when combined with a sliding window we can find objects in images in various locations.\n",
    "#### At the bottom of the pyramid, we have the original image at its original size (in terms of width and height). And at each subsequent layer, the image is resized (subsampled) and optionally smoothed (usually via Gaussian blurring). The image is progressively subsampled until some stopping criterion is met, which is normally when a minimum size has been reached and no further subsampling needs to take place.\n",
    "#### REf: https://pyimagesearch.com/2015/03/16/image-pyramids-with-python-and-opencv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688c4a9",
   "metadata": {},
   "source": [
    "#### Combined with image pyramids, sliding windows allow us to localize objects at different locations and multiple scales of the input image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca24e5e",
   "metadata": {},
   "source": [
    "### We need to define two parameters\n",
    "### scale: by what scale images are subsample\n",
    "### minimum size of the image for stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we are going to run teh sliding window for each scale of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "\t# yield the original image\n",
    "\tyield image\n",
    "\t# keep looping over the image pyramid\n",
    "\twhile True:\n",
    "\t\t# compute the dimensions of the next image in the pyramid\n",
    "\t\tw = int(image.shape[1] / scale)\n",
    "\t\timage = imutils.resize(image, width=w)\n",
    "\t\t# if the resized image does not meet the supplied minimum\n",
    "\t\t# size, then stop constructing the pyramid\n",
    "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "\t\t\tbreak\n",
    "\t\t# yield the next image in the pyramid\n",
    "\t\tyield image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables used for the object detection procedure\n",
    "base_img_width = 600\n",
    "pyr_scale = 1.5\n",
    "min_size=(224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(image_path)\n",
    "print(\"[INFO] image shape: \",img.shape)\n",
    "img = imutils.resize(img, width=img_resize_width)\n",
    "img_h,img_w=img.shape[:2]\n",
    "print(\"[INFO] image shape after resize: \",img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa959f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_pr in image_pyramid(img,scale=1.5,minSize=min_size):\n",
    "    print(img_pr.shape)\n",
    "    cv2.imshow(\"img_pr\",img_pr)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171942ed",
   "metadata": {},
   "source": [
    "### Combine image pyramid with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8827c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_size=(224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the image pyramid\n",
    "pyramid = image_pyramid(img, scale=pyr_scale, minSize=roi_size)\n",
    "# initialize two lists, one to hold the ROIs generated from the image\n",
    "# pyramid and sliding window, and another list used to store the\n",
    "# (x, y)-coordinates of where the ROI was in the original image\n",
    "rois = []\n",
    "locs = []\n",
    "# time how long it takes to loop over the image pyramid layers and\n",
    "# sliding window locations\n",
    "start = time.time()\n",
    "# loop over the image pyramid\n",
    "for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "    scale = img_w / float(image.shape[1])\n",
    "    # for each layer of the image pyramid, loop over the sliding\n",
    "    # window locations\n",
    "    for (x, y, roi_cropped) in sliding_window(image, step, roi_size):\n",
    "        # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "        # *original* image dimensions\n",
    "        org_x,org_y,org_w,org_h=x,y,roi_size[0],roi_size[1]\n",
    "        print(\"org: \",org_x,org_y,org_w,org_h)\n",
    "        x = int(x * scale)\n",
    "        y = int(y * scale)\n",
    "        w = int(roi_size[0] * scale)\n",
    "        h = int(roi_size[1] * scale)\n",
    "        print(\"scaled: \", x,y,w,h)\n",
    "        # take the ROI and preprocess it so we can later classify\n",
    "        # the region using Keras/TensorFlow\n",
    "        roi = cv2.resize(roi_cropped, model_input_size)\n",
    "        roi = img_to_array(roi)\n",
    "        roi = preprocess_input(roi)\n",
    "        # update our list of ROIs and associated coordinates\n",
    "        rois.append(roi)\n",
    "        locs.append((x, y, x + w, y + h))\n",
    "        \n",
    "        img_vis=image.copy()\n",
    "        img_roi=roi_cropped.copy()\n",
    "        cv2.rectangle(img_vis, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "        cv2.imshow(\"img\", img_vis)\n",
    "        cv2.imshow(\"cropped_roi\", img_roi)\n",
    "        cv2.waitKey(30)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue scaled\n",
    "# green original\n",
    "img_test=image.copy()\n",
    "cv2.rectangle(img_test, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "cv2.rectangle(img_test, (org_x, org_y), (org_x+org_w, org_y+org_h), (0,255,0), 2)\n",
    "\n",
    "img_test_org=img.copy()\n",
    "cv2.rectangle(img_test_org, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "cv2.rectangle(img_test_org, (org_x, org_y), (org_x+org_w, org_y+org_h), (0,255,0), 2)\n",
    "\n",
    "cv2.imshow(\"img_resized\", img_test)\n",
    "cv2.imshow(\"img_original\", img_test_org)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test_org=img.copy()\n",
    "cv2.rectangle(img_test_org, (x, y), (x+w, y+h), (255,0,0), 2)\n",
    "\n",
    "cv2.rectangle(img_test_org, (org_x, org_y), (org_x+org_w, org_y+org_h), (0,255,0), 2)\n",
    "cv2.imshow(\"img2\", img_test_org)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop with the original coord from resized image\n",
    "# crop with the scaled coord from original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e140d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized_crop=image[org_y:org_y + org_w, org_x:org_x + org_h]\n",
    "img_org_crop=img[y:y + w, x:x + h]\n",
    "img_org_crop_org_coord=img[org_y:org_y + org_w, org_x:org_x + org_h]\n",
    "cv2.imshow(\"img_resized_crop\", img_resized_crop)\n",
    "cv2.imshow(\"img_org_crop\", img_org_crop)\n",
    "cv2.imshow(\"img_org_crop_org_coord\", img_org_crop_org_coord)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3e877",
   "metadata": {},
   "source": [
    "### Drawbacks of the above approach\n",
    "### 1. It is a very slow approcah to slide windows and if we add image pyramid, it further slows. \n",
    "### 2. Very tough to estimate the window size and stride, This however determines the accuracy of the object detector.\n",
    "### 3. This is not end to end trainable, whihc means we cannot correct the error on the boundin box location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17fe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
